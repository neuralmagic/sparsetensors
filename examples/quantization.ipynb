{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuantizationConfig Application ##\n",
    "\n",
    "`QuantizationConfig` allows for compressing the model on disk by reducing the precision of weights e.g. from float16 to int8.\n",
    "\n",
    "In order to save a compressed (quantized)\n",
    "\n",
    "1. Create a \"vanilla\" model. For that purpose we are using a `TinyLlaMa` model\n",
    "2. Define the arguments of the `QuantizationConfig`\n",
    "3. Use the function `apply_quantization_config` to modify the model to, for the relevant weight matrices, add parameters that simulate the quantize and dequantize operations (scale and zero point).\n",
    "4. Calibrate the scale and zero point through few forward passes of the calibration data\n",
    "5. Using the obtained scales and zero points to quantize the weight matrices to `int8` representation.\n",
    "\n",
    "The example below shows how to quantize the model. It also demonstrates the benefits of the quantization over \"dense\" representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nm/drive0/damian/compressed-tensors/.venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:186: UserWarning: Field name \"registry_requires_subclass\" shadows an attribute in parent \"RegistryMixin\"; \n",
      "  warnings.warn(\n",
      "/nm/drive0/damian/compressed-tensors/.venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:186: UserWarning: Field name \"registry_requires_subclass\" shadows an attribute in parent \"CompressionConfig\"; \n",
      "  warnings.warn(\n",
      "/nm/drive0/damian/compressed-tensors/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from compressed_tensors.quantization import QuantizationConfig, apply_quantization_config, freeze_module_quantization\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"neuralmagic/llama2.c-stories110M-pruned50\" # model to quantize and calibrate\n",
    "dataset_name = \"garage-bAInd/Open-Platypus\" # dataset to calibrate on\n",
    "num_calibration_samples = 1 # num calibration samples to calibrate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: layers.0.self_attn.q_proj has been quantized: False\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.model.named_modules():\n",
    "    module_type = module.__class__.__name__\n",
    "    if module_type == \"Linear\":\n",
    "        is_quantized = hasattr(module, \"quantization_scheme\")\n",
    "        print(f\"Module: {name} has been quantized: {is_quantized}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the quantization configuration\n",
    "quantization_config = QuantizationConfig(\n",
    "        # \"fakequant\" means that the weights are still in their original format\n",
    "        # (e.g. float16), but quantization is emulated by adding scales/zeropoints \n",
    "        # to the model state_dict\n",
    "        format = \"fakequant\",\n",
    "        # \"initialize\" means that scale/zeropoints and observers have been attached \n",
    "        # to the layer but are set to dummy values (not yet calibrated)\n",
    "        quantization_status = \"calibration\",\n",
    "        config_groups ={\n",
    "        # \"group_1\" acts on all the nn.Linear layers of the model\n",
    "        # it quantizes the weights to 8 bits (symmetric, so zero_point = 0) \n",
    "        # it quantizes the input activations to 8 bits (asymmetric, so zero_point != 0)\n",
    "        \"group_1\": {\n",
    "            \"weights\": {\n",
    "                \"num_bits\": 8,\n",
    "                \"type\": \"int\",\n",
    "                \"symmetric\": True,\n",
    "                \"strategy\": \"tensor\"\n",
    "            },\n",
    "            \"input_activations\": {\n",
    "                \"num_bits\": 8,\n",
    "                \"type\": \"int\",\n",
    "                \"symmetric\": False,\n",
    "                \"strategy\": \"tensor\"\n",
    "            },\n",
    "            \"targets\": [\"Linear\"]\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply the quantization configuration\n",
    "apply_quantization_config(model, quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: layers.0.self_attn.q_proj has been quantized: True\n",
      "Input activation scale: Parameter containing:\n",
      "tensor([])\n",
      "Input activation zero point: Parameter containing:\n",
      "tensor([], dtype=torch.int64)\n",
      "Weight scale: Parameter containing:\n",
      "tensor([])\n",
      "Weight zero point Parameter containing:\n",
      "tensor([], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.model.named_modules():\n",
    "    module_type = module.__class__.__name__\n",
    "    if module_type == \"Linear\":\n",
    "        is_quantized = hasattr(module, \"quantization_scheme\")\n",
    "        print(f\"Module: {name} has been quantized: {is_quantized}\")\n",
    "        print(f\"Input activation scale: {module.input_scale}\")\n",
    "        print(f\"Input activation zero point: {module.input_zero_point}\")\n",
    "        print(f\"Weight scale: {module.weight_scale}\")\n",
    "        print(f\"Weight zero point {module.weight_zero_point}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:23<00:00, 83.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Calibrate the quantized layers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = load_dataset(dataset_name, split='train', streaming=True)\n",
    "\n",
    "# run calibration\n",
    "for idx, sample in tqdm(enumerate(dataset),  total = num_calibration_samples):\n",
    "    sample = tokenizer(sample['output'], return_tensors=\"pt\")\n",
    "    _ = model(**sample)\n",
    "\n",
    "    if idx >= num_calibration_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: layers.0.self_attn.q_proj has been quantized: True\n",
      "Input activation scale: 0.02618597261607647\n",
      "Input activation zero point: 85\n",
      "Weight scale: 0.008901744149625301\n",
      "Weight zero point 0\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.model.named_modules():\n",
    "    module_type = module.__class__.__name__\n",
    "    if module_type == \"Linear\":\n",
    "        is_quantized = hasattr(module, \"quantization_scheme\")\n",
    "        print(f\"Module: {name} has been quantized: {is_quantized}\")\n",
    "        print(f\"Input activation scale: {module.input_scale.item()}\")\n",
    "        print(f\"Input activation zero point: {module.input_zero_point.item()}\")\n",
    "        print(f\"Weight scale: {module.weight_scale.item()}\")\n",
    "        print(f\"Weight zero point {module.weight_zero_point.item()}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(freeze_module_quantization)\n",
    "quantization_config.format = \"compressed\"\n",
    "apply_quantization_config(model, quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: layers.0.self_attn.q_proj has been quantized: True\n",
      "Input activation scale: Parameter containing:\n",
      "tensor([])\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000, -0.0712,  ...,  0.0000, -0.0178,  0.0000],\n",
      "        [ 0.0000,  0.0979,  0.1246,  ...,  0.1869,  0.0801, -0.1602],\n",
      "        [ 0.0000, -0.0712,  0.0000,  ..., -0.2492, -0.1157,  0.2671],\n",
      "        ...,\n",
      "        [-0.0801, -0.0801,  0.0000,  ...,  0.0000,  0.0000,  0.1068],\n",
      "        [-0.1157, -0.0801, -0.0712,  ...,  0.0890,  0.0712,  0.0890],\n",
      "        [-0.1068,  0.0445,  0.0000,  ...,  0.0712,  0.1246,  0.0534]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([]), Parameter containing:\n",
      "tensor([], dtype=torch.int64), Parameter containing:\n",
      "tensor([]), Parameter containing:\n",
      "tensor([], dtype=torch.int64)]\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.model.named_modules():\n",
    "    module_type = module.__class__.__name__\n",
    "    if module_type == \"Linear\":\n",
    "        is_quantized = hasattr(module, \"quantization_scheme\")\n",
    "        print(f\"Module: {name} has been quantized: {is_quantized}\")\n",
    "        print(f\"Input activation scale: {module.input_scale}\")\n",
    "        print(list(module.parameters()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
