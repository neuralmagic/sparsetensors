{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuantizationConfig Application ##\n",
    "\n",
    "`QuantizationConfig` allows for compressing the model on disk by reducing the precision of weights e.g. from float16 to int8.\n",
    "\n",
    "In order to save a compressed (quantized)\n",
    "\n",
    "1. Create a \"vanilla\" model. For that purpose we are using a `TinyLlaMa` model\n",
    "2. Define the arguments of the `QuantizationConfig`\n",
    "3. Use the function `apply_quantization_config` to modify the model to, for the relevant weight matrices, add parameters that simulate the quantize and dequantize operations (scale and zero point).\n",
    "4. Calibrate the scale and zero point through few forward passes of the calibration data\n",
    "5. Using the obtained scales and zero points to quantize the weight matrices to `int8` representation.\n",
    "\n",
    "The example below shows how to quantize the model. It also demonstrates the benefits of the quantization over \"dense\" representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nm/drive0/damian/compressed-tensors/.venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:186: UserWarning: Field name \"registry_requires_subclass\" shadows an attribute in parent \"RegistryMixin\"; \n",
      "  warnings.warn(\n",
      "/nm/drive0/damian/compressed-tensors/.venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:186: UserWarning: Field name \"registry_requires_subclass\" shadows an attribute in parent \"CompressionConfig\"; \n",
      "  warnings.warn(\n",
      "/nm/drive0/damian/compressed-tensors/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from compressed_tensors.quantization import QuantizationConfig, apply_quantization_config, freeze_module_quantization, set_module_for_calibration\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"neuralmagic/llama2.c-stories110M-pruned50\" # model to quantize and calibrate\n",
    "dataset_name = \"roneneldan/TinyStories\" # dataset to calibrate on\n",
    "num_calibration_samples = 256 # num calibration samples to calibrate on\n",
    "batch_size = 32 # batch size for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: layers.0.self_attn.q_proj has been quantized: False\n"
     ]
    }
   ],
   "source": [
    "def is_first_linear_layer_quantized(model: \"torch.nn.Module\"):\n",
    "    \"\"\"\n",
    "    Helper function that helps us determine if the first linear layer \n",
    "    in the model has been quantized (proxy for quantization state of the\n",
    "    whole model)\n",
    "    \"\"\"\n",
    "    for name, module in model.model.named_modules():\n",
    "        module_type = module.__class__.__name__\n",
    "        if module_type == \"Linear\":\n",
    "            is_quantized = hasattr(module, \"quantization_scheme\")\n",
    "            print(f\"Module: {name} has been quantized: {is_quantized}\")\n",
    "            if is_quantized:\n",
    "                print(f\"Input activation scale: {module.input_scale}\")\n",
    "                print(f\"Input activation zero point: {module.input_zero_point}\")\n",
    "                print(f\"Weight scale: {module.weight_scale}\")\n",
    "                print(f\"Weight zero point {module.weight_zero_point}\")\n",
    "            break\n",
    "is_first_linear_layer_quantized(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the quantization configuration\n",
    "quantization_config = QuantizationConfig(\n",
    "        # \"fakequant\" means that the weights are still in their original format\n",
    "        # (e.g. float16), but quantization is emulated by adding scales/zeropoints \n",
    "        # to the model state_dict\n",
    "        format = \"fakequant\",\n",
    "        # \"initialize\" means that scale/zeropoints and observers have been attached \n",
    "        # to the layer but are set to dummy values (not yet calibrated)\n",
    "        quantization_status = \"calibration\",\n",
    "        config_groups ={\n",
    "        # \"group_1\" acts on all the nn.Linear layers of the model\n",
    "        # it quantizes the weights to 8 bits (symmetric, so zero_point = 0) \n",
    "        # it quantizes the input activations to 8 bits (asymmetric, so zero_point != 0)\n",
    "        \"group_1\": {\n",
    "            \"weights\": {\n",
    "                \"num_bits\": 8,\n",
    "                \"type\": \"int\",\n",
    "                \"symmetric\": True,\n",
    "                \"strategy\": \"tensor\"\n",
    "            },\n",
    "            \"input_activations\": {\n",
    "                \"num_bits\": 8,\n",
    "                \"type\": \"int\",\n",
    "                \"symmetric\": False,\n",
    "                \"strategy\": \"tensor\"\n",
    "            },\n",
    "            \"targets\": [\"Linear\"]\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: layers.0.self_attn.q_proj has been quantized: True\n",
      "Input activation scale: Parameter containing:\n",
      "tensor([], device='cuda:0')\n",
      "Input activation zero point: Parameter containing:\n",
      "tensor([], device='cuda:0', dtype=torch.int64)\n",
      "Weight scale: Parameter containing:\n",
      "tensor([], device='cuda:0')\n",
      "Weight zero point Parameter containing:\n",
      "tensor([], device='cuda:0', dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply the quantization configuration and set the model for calibration\n",
    "apply_quantization_config(model, quantization_config)\n",
    "is_first_linear_layer_quantized(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "100%|██████████| 8/8 [00:09<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Calibrate the quantized layers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = load_dataset(dataset_name, split='train', streaming=True)\n",
    "dataset = dataset.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# # run calibration\n",
    "for idx, sample in tqdm(enumerate(data_loader),  total = num_calibration_samples // batch_size):\n",
    "    input_ids = torch.stack(sample[\"input_ids\"],axis=1).to(model.device)\n",
    "    attention_mask = torch.stack(sample[\"attention_mask\"],axis=1).to(model.device)\n",
    "\n",
    "    _ = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    if idx >= num_calibration_samples // batch_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: layers.0.self_attn.q_proj has been quantized: True\n",
      "Input activation scale: Parameter containing:\n",
      "tensor([0.0277], device='cuda:0')\n",
      "Input activation zero point: Parameter containing:\n",
      "tensor([69], device='cuda:0', dtype=torch.int8)\n",
      "Weight scale: Parameter containing:\n",
      "tensor([0.0089], device='cuda:0')\n",
      "Weight zero point Parameter containing:\n",
      "tensor(0, device='cuda:0', dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "model.apply(freeze_module_quantization)\n",
    "is_first_linear_layer_quantized(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(freeze_module_quantization)\n",
    "quantization_config.format = \"compressed\"\n",
    "apply_quantization_config(model, quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: layers.0.self_attn.q_proj has been quantized: True\n",
      "Input activation scale: Parameter containing:\n",
      "tensor([], device='cuda:0')\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000, -0.0712,  ...,  0.0000, -0.0178,  0.0000],\n",
      "        [ 0.0000,  0.0979,  0.1246,  ...,  0.1869,  0.0801, -0.1602],\n",
      "        [ 0.0000, -0.0712,  0.0000,  ..., -0.2492, -0.1157,  0.2671],\n",
      "        ...,\n",
      "        [-0.0801, -0.0801,  0.0000,  ...,  0.0000,  0.0000,  0.1068],\n",
      "        [-0.1157, -0.0801, -0.0712,  ...,  0.0890,  0.0712,  0.0890],\n",
      "        [-0.1068,  0.0445,  0.0000,  ...,  0.0712,  0.1246,  0.0534]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([], device='cuda:0'), Parameter containing:\n",
      "tensor([], device='cuda:0', dtype=torch.int64), Parameter containing:\n",
      "tensor([], device='cuda:0'), Parameter containing:\n",
      "tensor([], device='cuda:0', dtype=torch.int64)]\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.model.named_modules():\n",
    "    module_type = module.__class__.__name__\n",
    "    if module_type == \"Linear\":\n",
    "        is_quantized = hasattr(module, \"quantization_scheme\")\n",
    "        print(f\"Module: {name} has been quantized: {is_quantized}\")\n",
    "        print(f\"Input activation scale: {module.input_scale}\")\n",
    "        print(list(module.parameters()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
